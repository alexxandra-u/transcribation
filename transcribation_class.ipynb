{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oq2kKZXQRxVp"
      },
      "outputs": [],
      "source": [
        "!pip install faster_whisper\n",
        "!pip install git+https://github.com/m-bain/whisperx.git\n",
        "!pip install python-docx"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class for audio transcribation and diarization"
      ],
      "metadata": {
        "id": "6pfhaCrkR3Gw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from faster_whisper import WhisperModel\n",
        "from docx import Document\n",
        "import whisperx\n",
        "import gc\n",
        "\n",
        "\n",
        "class Audio():\n",
        "    def __init__(self, filename, hf_token, device ='cuda'):\n",
        "\n",
        "        \"\"\"\n",
        "        :param filename: name of audio/video\n",
        "        :param hf_token: HuggingFace token to access open-source models\n",
        "        :param device: cuda or cpu\n",
        "        \"\"\"\n",
        "\n",
        "        self.filename = filename\n",
        "\n",
        "        self.transcriber = WhisperModel(\"large-v2\", device=device)\n",
        "        self.transcribation_text = Document()\n",
        "\n",
        "        self.diarizer1 = whisperx.load_model(\"large-v2\", device, compute_type=\"float16\")\n",
        "        self.diarizer2 = whisperx.DiarizationPipeline(use_auth_token=hf_token, device=device)\n",
        "        self.diarization_text = Document()\n",
        "\n",
        "    def transcribe(self):\n",
        "        segments, info = self.transcriber.transcribe(self.filename)\n",
        "        for segment in segments:\n",
        "            self.transcribation_text.add_paragraph(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n",
        "            print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n",
        "\n",
        "    def download_transcribation(self):\n",
        "        if len(self.transcribation_text.paragraphs) == 0:\n",
        "            print(\"The transcribation wasn't made yet. Use transribe() first.\")\n",
        "            return\n",
        "        self.transcribation_text.save(self.filename.split(\".\")[0]+\".docx\")\n",
        "\n",
        "    def diarize(self, batch_size=16, device='cuda'):\n",
        "        audio = whisperx.load_audio(self.filename)\n",
        "        result =  self.diarizer1.transcribe(audio, batch_size=batch_size)\n",
        "        model_a, metadata = whisperx.load_align_model(language_code=result[\"language\"], device=device)\n",
        "        result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
        "        diarize_segments = self.diarizer2(audio)\n",
        "        result = whisperx.assign_word_speakers(diarize_segments, result)\n",
        "\n",
        "        prev_speaker, text = '', ''\n",
        "        start, end = 0, 0\n",
        "        for i in result['segments']:\n",
        "            if prev_speaker == '':\n",
        "                prev_speaker = i['speaker']\n",
        "                start = i['start']\n",
        "            elif prev_speaker == i['speaker']:\n",
        "                text += i['text'] + ' '\n",
        "            else:\n",
        "                end = i['end']\n",
        "                if text != '':\n",
        "                    self.diarization_text.add_paragraph(f'{prev_speaker}: {text.strip()}')\n",
        "                prev_speaker = i['speaker']\n",
        "                text = i['text']\n",
        "\n",
        "    def download_diarization(self):\n",
        "        if len(self.diarization_text.paragraphs) == 0:\n",
        "            print(\"The diarization wasn't made yet. Use diarize() first.\")\n",
        "            return\n",
        "        self.diarization_text.save(self.filename.split(\".\")[0]+\".docx\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGXYZTz2Sa2T",
        "outputId": "4f6b7bcf-a31f-421e-aa10-a6cf2dd95312"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyannote/audio/core/io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
            "  torchaudio.set_audio_backend(\"soundfile\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Usage example"
      ],
      "metadata": {
        "id": "I6i6czD8SL6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_audio = Audio('video_name.mp4')\n",
        "my_audio.transcribe()\n",
        "my_audio.download_transcribation()"
      ],
      "metadata": {
        "id": "4jLvrFjtR2fT"
      },
      "execution_count": 1,
      "outputs": []
    }
  ]
}